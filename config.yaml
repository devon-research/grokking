# Relevant for the data and model...
modular_base: 113
# Relevant for the data...
train_fraction: 0.3
use_equals_symbol: true
poisoned_fraction: 0.01
poisoning_scheme: 17Fixed
# Relevant for the model...
model: NandaTransformer
embed_dim: 128
mlp_hidden_dim: 512
num_attention_heads: 4
# Relevant for the optimization...
random_seed: 23093
optimizer: AdamW
loss_function: CrossEntropy
learning_rate: 0.001
full_batch: true
batch_size: -1
n_epochs: 50001
weight_decay: 1.0
beta_1: 0.9
beta_2: 0.98
# Relevant for logging...
logit_dtype: float64
validate_every: 100
save_checkpoints: true